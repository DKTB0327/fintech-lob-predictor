{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103372,"status":"ok","timestamp":1743490071893,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"},"user_tz":-480},"id":"zGWCP11-fJy6","outputId":"58c4dbfc-f30a-4c36-a2ea-e014e7f32924"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":4870,"status":"ok","timestamp":1743490076749,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"},"user_tz":-480},"id":"T7eVM3tIroSx","outputId":"7c1cc68c-bd0a-48bd-8ca1-1c92777481a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-geometric\n","Successfully installed torch-geometric-2.6.1\n"]}],"source":["!pip install torch-geometric"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, matthews_corrcoef, cohen_kappa_score"],"metadata":{"id":"x3WpRquAK_m4","executionInfo":{"status":"ok","timestamp":1743490108218,"user_tz":-480,"elapsed":31465,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(y_true, y_pred, label_encoder, model_name=\"Model\"):\n","    metrics = {}\n","    metrics['Balanced Accuracy'] = balanced_accuracy_score(y_true, y_pred)\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n","    metrics['Weighted Precision'] = precision\n","    metrics['Weighted Recall'] = recall\n","    metrics['Weighted F1'] = f1\n","    metrics['MCC'] = matthews_corrcoef(y_true, y_pred)\n","    metrics['Cohen Kappa'] = cohen_kappa_score(y_true, y_pred)\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n","    classes = label_encoder.classes_\n","    for i, cls in enumerate(classes):\n","        metrics[f'Precision_{cls}'] = precision[i]\n","        metrics[f'Recall_{cls}'] = recall[i]\n","        metrics[f'F1_{cls}'] = f1[i]\n","\n","    print(f\"{model_name} Performance:\")\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n","    print()\n","    return metrics\n","\n","all_metrics = {}"],"metadata":{"id":"HWmRr9m1Taaj","executionInfo":{"status":"ok","timestamp":1743490135374,"user_tz":-480,"elapsed":57,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.preprocessing import StandardScaler\n","\n","def load_and_preprocess_data(data_path, sample_fraction=0.25):\n","    \"\"\"\n","    Load and preprocess financial data for GNN training with stratified sampling.\n","\n","    Args:\n","        data_path (str): Path to the CSV file.\n","        sample_fraction (float): Fraction of data to sample (default: 0.25).\n","\n","    Returns:\n","        df (pd.DataFrame): Preprocessed DataFrame.\n","        X (np.ndarray): Normalized feature matrix.\n","        y (np.ndarray): Target array with values [0, 1, 2].\n","    \"\"\"\n","    # Load data\n","    df = pd.read_csv(data_path)\n","    print(f\"Initial rows: {len(df)}\")\n","\n","    # Debug: Inspect initial state\n","    print(\"Columns in the dataset:\", df.columns.tolist())\n","    if df.empty:\n","        raise ValueError(\"Error: The dataset is empty. Please check the file path and contents.\")\n","\n","    # Define movement mapping\n","    movement_mapping = {'q_-1': 0, 'q_0': 1, 'q_+1': 2}\n","\n","    # Map Movement and filter invalid values\n","    print(\"Unique values in 'Movement' (before):\", df['Movement'].unique())\n","    df['Movement'] = df['Movement'].map(movement_mapping)\n","    valid_idx = df['Movement'].isin([0, 1, 2])\n","    print(\"Rows dropped due to invalid 'Movement':\", (~valid_idx).sum())\n","    df = df[valid_idx]\n","\n","    # Stratified sampling using StratifiedShuffleSplit\n","    sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - sample_fraction, random_state=42)\n","    for train_idx, _ in sss.split(df, df['Movement']):\n","        df = df.iloc[train_idx]\n","    print(f\"Rows after subsampling: {len(df)}\")\n","\n","    # Target variable\n","    y = df['Movement'].values.astype(np.int64)  # [0, 1, 2]\n","    print(\"Unique values in y:\", np.unique(y))\n","\n","    # Define feature columns\n","    feature_cols = [\n","        'BidPrice1', 'AskPrice1', 'BidSize1', 'AskSize1',\n","        'Spread', 'Imbalance1', 'MidPrice_Volatility_10', 'CumulativeOrderFlow',\n","        'TotalAskSize', 'TotalBidSize', 'DepthRatio', 'AskVWAP', 'BidVWAP',\n","        'VWAP_Imbalance', 'LogReturn', 'RealizedVol_1sec'\n","    ]\n","\n","    # Handle missing features\n","    missing_cols = [col for col in feature_cols if col not in df.columns]\n","    if missing_cols:\n","        raise ValueError(f\"Error: Missing feature columns: {missing_cols}\")\n","\n","    # Extract and preprocess features\n","    X = df[feature_cols].fillna(0).values\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    return df, X, y"],"metadata":{"id":"uQL1ILyqLGcS","executionInfo":{"status":"ok","timestamp":1743490730620,"user_tz":-480,"elapsed":37,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xIABmw4MGX-s","executionInfo":{"status":"ok","timestamp":1743490902701,"user_tz":-480,"elapsed":45,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}}},"outputs":[],"source":["# Step 2: Graph Construction\n","from sklearn.neighbors import NearestNeighbors\n","\n","def construct_graph_knn(df, X, y, k=10):\n","    num_nodes = len(df)\n","    node_features = torch.tensor(X, dtype=torch.float)\n","    labels = torch.tensor(y, dtype=torch.long)\n","    timestamps = df['Time'].values.reshape(-1, 1)  # Reshape for sklearn\n","\n","    # Find k nearest neighbors in time\n","    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='auto').fit(timestamps)\n","    distances, indices = nbrs.kneighbors(timestamps)\n","\n","    # Build edge index\n","    edge_index = []\n","    for i in range(num_nodes):\n","        for j in indices[i, 1:]:  # Skip self (indices[i, 0] == i)\n","            edge_index.append([i, j])\n","            edge_index.append([j, i])  # Undirected graph\n","\n","    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n","    data = Data(x=node_features, edge_index=edge_index, y=labels)\n","    return data"]},{"cell_type":"code","source":["# Step 3: GNN Model\n","class GNNModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(GNNModel, self).__init__()\n","        self.conv1 = GCNConv(input_dim, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n","        self.conv3 = GCNConv(hidden_dim, output_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu(self.conv2(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","\n","# Step 4: Training and Evaluation\n","def train_model(data, model, optimizer, criterion, num_epochs=100):\n","    best_test_loss = float('inf')\n","    patience = 10\n","    patience_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(data)\n","        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            test_loss = criterion(out[data.test_mask], data.y[data.test_mask])\n","\n","        if epoch % 10 == 0:\n","            print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\")\n","\n","        if test_loss < best_test_loss:\n","            best_test_loss = test_loss\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered\")\n","                break"],"metadata":{"id":"tLQ2RJCnLMO5","executionInfo":{"status":"ok","timestamp":1743491543710,"user_tz":-480,"elapsed":7,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Ensure the label encoder maps classes correctly to q_-1, q_0, q_+1\n","def evaluate_model(data, model, label_encoder, model_name=\"GNN\"):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        pred = out.argmax(dim=1)  # Predicted class indices\n","\n","        # Convert to NumPy for sklearn metrics\n","        y_train_true = data.y[data.train_mask].cpu().numpy()\n","        y_train_pred = pred[data.train_mask].cpu().numpy()\n","        y_test_true = data.y[data.test_mask].cpu().numpy()\n","        y_test_pred = pred[data.test_mask].cpu().numpy()\n","\n","        # Compute metrics for train and test sets\n","        train_metrics = compute_metrics(y_train_true, y_train_pred, label_encoder, model_name=f\"{model_name} (Train)\")\n","        test_metrics = compute_metrics(y_test_true, y_test_pred, label_encoder, model_name=f\"{model_name} (Test)\")\n","\n","        # Store in all_metrics with unique keys\n","        all_metrics[f'{model_name}_Train'] = train_metrics\n","        all_metrics[f'{model_name}_Test'] = test_metrics\n","\n","# Function to create the evaluation matrix\n","def create_evaluation_matrix(all_metrics):\n","    # Define the metrics to include in the table (matching your provided table)\n","    metrics_order = [\n","        'Balanced Accuracy', 'Weighted Precision', 'Weighted Recall', 'Weighted F1',\n","        'MCC', 'Cohen Kappa',\n","        'Precision_q_-1', 'Recall_q_-1', 'F1_q_-1',\n","        'Precision_q_0', 'Recall_q_0', 'F1_q_0',\n","        'Precision_q_+1', 'Recall_q_+1', 'F1_q_+1',\n","        'Precision_0', 'Recall_0', 'F1_0',\n","        'Precision_1', 'Recall_1', 'F1_1',\n","        'Precision_2', 'Recall_2', 'F1_2'\n","    ]\n","\n","    # Create a DataFrame with metrics as rows and models as columns\n","    model_names = list(all_metrics.keys())\n","    matrix = pd.DataFrame(index=metrics_order, columns=model_names)\n","\n","    # Populate the DataFrame\n","    for model_name in model_names:\n","        metrics = all_metrics[model_name]\n","        for metric in metrics_order:\n","            if metric in metrics:\n","                matrix.loc[metric, model_name] = metrics[metric]\n","            else:\n","                matrix.loc[metric, model_name] = 'N/A'\n","\n","    return matrix\n","\n","# Model with LSTM\n","class GNNWithLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n","        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, output_dim)\n","    def forward(self, data):\n","        x = data.x.unsqueeze(0)\n","        x, _ = self.lstm(x)\n","        x = x.squeeze(0)\n","        x = F.relu(self.conv1(x, data.edge_index))\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv2(x, data.edge_index)\n","        return x\n","\n","\n","# Update the main execution to use the modified evaluate_model\n","if __name__ == \"__main__\":\n","    # Clear all_metrics to start fresh\n","    all_metrics = {}\n","\n","    # Load balanced dataset\n","    data_path = \"/content/drive/MyDrive/FYP_Dataset/LOBSTER_SampleFile_AAPL_2012-06-21_5/AAPL_2012-06-21_balanced_dataset.csv\"\n","    df, X, y = load_and_preprocess_data(data_path, sample_fraction=0.1)  # Smaller sample for speed\n","\n","    # Enhanced graph construction\n","    feature_cols = ['Time', 'MidPriceChange_Cumsum10', 'Imbalance1']\n","    combined = StandardScaler().fit_transform(df[feature_cols])\n","    nbrs = NearestNeighbors(n_neighbors=21).fit(combined)\n","    distances, indices = nbrs.kneighbors(combined)\n","    edge_index = torch.tensor([[i, j] for i in range(len(df)) for j in indices[i, 1:]], dtype=torch.long).t()\n","    data = Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index, y=torch.tensor(y, dtype=torch.long))\n","\n","    # Split\n","    train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y, random_state=42)\n","    data.train_mask = torch.zeros(len(y), dtype=torch.bool); data.train_mask[train_idx] = True\n","    data.test_mask = torch.zeros(len(y), dtype=torch.bool); data.test_mask[test_idx] = True\n","\n","    # Second GNN Model (GNNWithLSTM)\n","    df, X, y = load_and_preprocess_data(data_path, sample_fraction=0.1)\n","\n","    feature_cols = ['Time', 'MidPriceChange_Cumsum10', 'Imbalance1']\n","    combined = StandardScaler().fit_transform(df[feature_cols])\n","    nbrs = NearestNeighbors(n_neighbors=21).fit(combined)\n","    distances, indices = nbrs.kneighbors(combined)\n","    edge_index = torch.tensor([[i, j] for i in range(len(df)) for j in indices[i, 1:]], dtype=torch.long).t()\n","    data = Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index, y=torch.tensor(y, dtype=torch.long))\n","\n","    train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y, random_state=42)\n","    data.train_mask = torch.zeros(len(y), dtype=torch.bool)\n","    data.test_mask = torch.zeros(len(y), dtype=torch.bool)\n","    data.train_mask[train_idx] = True\n","    data.test_mask[test_idx] = True\n","\n","    model = GNNWithLSTM(X.shape[1], 128, 3)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    class_weights = 1.0 / torch.tensor(np.bincount(y), dtype=torch.float)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n","    train_model(data, model, optimizer, criterion, num_epochs=200)\n","    evaluate_model(data, model, label_encoder, model_name=\"GNNWithLSTM\")\n","\n","    # Create and display the evaluation matrix\n","    evaluation_matrix = create_evaluation_matrix(all_metrics)\n","    print(\"\\nEvaluation Matrix:\")\n","    print(evaluation_matrix)\n","\n","    # Optionally, save to CSV\n","    evaluation_matrix.to_csv('/content/drive/MyDrive/FYP_Dataset/evaluation_matrix.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbSlqjUCJGqK","executionInfo":{"status":"ok","timestamp":1743492675141,"user_tz":-480,"elapsed":1127854,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}},"outputId":"ae76c658-0252-4632-e93a-23318de912f1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial rows: 711705\n","Columns in the dataset: ['AskPrice1', 'AskSize1', 'BidPrice1', 'BidSize1', 'AskPrice2', 'AskSize2', 'BidPrice2', 'BidSize2', 'AskPrice3', 'AskSize3', 'BidPrice3', 'BidSize3', 'AskPrice4', 'AskSize4', 'BidPrice4', 'BidSize4', 'AskPrice5', 'AskSize5', 'BidPrice5', 'BidSize5', 'MidPrice', 'Time', 'MidPriceChange', 'Spread', 'Imbalance1', 'Imbalance2', 'Imbalance3', 'Imbalance4', 'Imbalance5', 'AskPriceDiff1', 'BidPriceDiff1', 'AskPriceDiff2', 'BidPriceDiff2', 'AskPriceDiff3', 'BidPriceDiff3', 'AskPriceDiff4', 'BidPriceDiff4', 'MidPriceChange_Lag1', 'MidPriceChange_Lag5', 'MidPriceChange_Lag10', 'EventCount_Type1', 'EventCount_Type2', 'EventCount_Type3', 'EventCount_Type4', 'EventCount_Type5', 'MidPrice_Volatility_10', 'CumulativeOrderFlow', 'MidPrice_MA10', 'AskSize1_MA10', 'BidSize1_MA10', 'TimeWeightedImbalance1', 'EventIntensity', 'AskPrice1_Relative', 'BidPrice1_Relative', 'AskPrice2_Relative', 'BidPrice2_Relative', 'AskPrice3_Relative', 'BidPrice3_Relative', 'AskPrice4_Relative', 'BidPrice4_Relative', 'AskPrice5_Relative', 'BidPrice5_Relative', 'TotalAskSize', 'TotalBidSize', 'DepthRatio', 'MidPriceChange_Cumsum10', 'MidPrice_EMA5', 'MidPrice_EMA20', 'AskVWAP', 'BidVWAP', 'VWAP_Imbalance', 'LogReturn', 'RealizedVol_1sec', 'TimeSinceOpen', 'Movement']\n","Unique values in 'Movement' (before): ['q_0' 'q_-1' 'q_+1']\n","Rows dropped due to invalid 'Movement': 0\n","Rows after subsampling: 71170\n","Unique values in y: [0 1 2]\n","Initial rows: 711705\n","Columns in the dataset: ['AskPrice1', 'AskSize1', 'BidPrice1', 'BidSize1', 'AskPrice2', 'AskSize2', 'BidPrice2', 'BidSize2', 'AskPrice3', 'AskSize3', 'BidPrice3', 'BidSize3', 'AskPrice4', 'AskSize4', 'BidPrice4', 'BidSize4', 'AskPrice5', 'AskSize5', 'BidPrice5', 'BidSize5', 'MidPrice', 'Time', 'MidPriceChange', 'Spread', 'Imbalance1', 'Imbalance2', 'Imbalance3', 'Imbalance4', 'Imbalance5', 'AskPriceDiff1', 'BidPriceDiff1', 'AskPriceDiff2', 'BidPriceDiff2', 'AskPriceDiff3', 'BidPriceDiff3', 'AskPriceDiff4', 'BidPriceDiff4', 'MidPriceChange_Lag1', 'MidPriceChange_Lag5', 'MidPriceChange_Lag10', 'EventCount_Type1', 'EventCount_Type2', 'EventCount_Type3', 'EventCount_Type4', 'EventCount_Type5', 'MidPrice_Volatility_10', 'CumulativeOrderFlow', 'MidPrice_MA10', 'AskSize1_MA10', 'BidSize1_MA10', 'TimeWeightedImbalance1', 'EventIntensity', 'AskPrice1_Relative', 'BidPrice1_Relative', 'AskPrice2_Relative', 'BidPrice2_Relative', 'AskPrice3_Relative', 'BidPrice3_Relative', 'AskPrice4_Relative', 'BidPrice4_Relative', 'AskPrice5_Relative', 'BidPrice5_Relative', 'TotalAskSize', 'TotalBidSize', 'DepthRatio', 'MidPriceChange_Cumsum10', 'MidPrice_EMA5', 'MidPrice_EMA20', 'AskVWAP', 'BidVWAP', 'VWAP_Imbalance', 'LogReturn', 'RealizedVol_1sec', 'TimeSinceOpen', 'Movement']\n","Unique values in 'Movement' (before): ['q_0' 'q_-1' 'q_+1']\n","Rows dropped due to invalid 'Movement': 0\n","Rows after subsampling: 71170\n","Unique values in y: [0 1 2]\n","Epoch 0, Train Loss: 1.0918, Test Loss: 1.0915\n","Epoch 10, Train Loss: 1.0481, Test Loss: 1.0502\n","Epoch 20, Train Loss: 0.9913, Test Loss: 0.9946\n","Epoch 30, Train Loss: 0.9567, Test Loss: 0.9592\n","Epoch 40, Train Loss: 0.9321, Test Loss: 0.9339\n","Epoch 50, Train Loss: 0.9138, Test Loss: 0.9144\n","Epoch 60, Train Loss: 0.9000, Test Loss: 0.9015\n","Epoch 70, Train Loss: 0.8891, Test Loss: 0.8909\n","Epoch 80, Train Loss: 0.8796, Test Loss: 0.8824\n","Epoch 90, Train Loss: 0.8712, Test Loss: 0.8730\n","Epoch 100, Train Loss: 0.8664, Test Loss: 0.8687\n","Epoch 110, Train Loss: 0.8623, Test Loss: 0.8629\n","Epoch 120, Train Loss: 0.8597, Test Loss: 0.8602\n","Epoch 130, Train Loss: 0.8577, Test Loss: 0.8590\n","Epoch 140, Train Loss: 0.8564, Test Loss: 0.8562\n","Epoch 150, Train Loss: 0.8554, Test Loss: 0.8560\n","Epoch 160, Train Loss: 0.8543, Test Loss: 0.8550\n","Early stopping triggered\n","GNNWithLSTM (Train) Performance:\n","Balanced Accuracy: 0.6175\n","Weighted Precision: 0.6192\n","Weighted Recall: 0.6175\n","Weighted F1: 0.6034\n","MCC: 0.4357\n","Cohen Kappa: 0.4263\n","Precision_q_-1: 0.6044\n","Recall_q_-1: 0.7657\n","F1_q_-1: 0.6756\n","Precision_q_0: 0.6249\n","Recall_q_0: 0.3704\n","F1_q_0: 0.4651\n","Precision_q_+1: 0.6283\n","Recall_q_+1: 0.7165\n","F1_q_+1: 0.6695\n","\n","GNNWithLSTM (Test) Performance:\n","Balanced Accuracy: 0.6166\n","Weighted Precision: 0.6183\n","Weighted Recall: 0.6166\n","Weighted F1: 0.6017\n","MCC: 0.4346\n","Cohen Kappa: 0.4248\n","Precision_q_-1: 0.6078\n","Recall_q_-1: 0.7642\n","F1_q_-1: 0.6771\n","Precision_q_0: 0.6256\n","Recall_q_0: 0.3638\n","F1_q_0: 0.4600\n","Precision_q_+1: 0.6215\n","Recall_q_+1: 0.7218\n","F1_q_+1: 0.6679\n","\n","\n","Evaluation Matrix:\n","                   GNNWithLSTM_Train GNNWithLSTM_Test\n","Balanced Accuracy           0.617531         0.616559\n","Weighted Precision          0.619194         0.618298\n","Weighted Recall             0.617535         0.616552\n","Weighted F1                  0.60339         0.601657\n","MCC                         0.435728         0.434627\n","Cohen Kappa                 0.426301         0.424831\n","Precision_q_-1              0.604383         0.607777\n","Recall_q_-1                 0.765741         0.764173\n","F1_q_-1                     0.675561         0.677061\n","Precision_q_0               0.624856         0.625589\n","Recall_q_0                  0.370376         0.363751\n","F1_q_0                      0.465081         0.460021\n","Precision_q_+1              0.628344         0.621528\n","Recall_q_+1                 0.716476         0.721754\n","F1_q_+1                     0.669522         0.667902\n","Precision_0                      N/A              N/A\n","Recall_0                         N/A              N/A\n","F1_0                             N/A              N/A\n","Precision_1                      N/A              N/A\n","Recall_1                         N/A              N/A\n","F1_1                             N/A              N/A\n","Precision_2                      N/A              N/A\n","Recall_2                         N/A              N/A\n","F1_2                             N/A              N/A\n"]}]},{"cell_type":"code","source":["def evaluate_model(data, model, label_encoder):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        pred = out.argmax(dim=1)  # Predicted class indices\n","\n","        # Convert to NumPy for sklearn metrics\n","        y_train_true = data.y[data.train_mask].cpu().numpy()\n","        y_train_pred = pred[data.train_mask].cpu().numpy()\n","        y_test_true = data.y[data.test_mask].cpu().numpy()\n","        y_test_pred = pred[data.test_mask].cpu().numpy()\n","\n","        # Compute metrics for train and test sets\n","        train_metrics = compute_metrics(y_train_true, y_train_pred, label_encoder, model_name=\"GNN (Train)\")\n","        test_metrics = compute_metrics(y_test_true, y_test_pred, label_encoder, model_name=\"GNN (Test)\")\n","\n","        # Store in all_metrics\n","        all_metrics['GNN_Train'] = train_metrics\n","        all_metrics['GNN_Test'] = test_metrics"],"metadata":{"id":"ILGUFsce1dPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Main Execution\n","if __name__ == \"__main__\":\n","    data_path = '/content/drive/MyDrive/FYP_Dataset/LOBSTER_SampleFile_AAPL_2012-06-21_5/AAPL_2012-06-21_balanced_dataset.csv'\n","    df, X, y = load_and_preprocess_data(data_path, sample_fraction=0.25)\n","\n","    class_counts = np.bincount(y)\n","    if len(class_counts) != 3:\n","        raise ValueError(f\"Error: Expected 3 classes, found {len(class_counts)}: {class_counts}\")\n","\n","    class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n","    class_weights = class_weights / class_weights.sum()  # Normalize\n","    class_weights[class_weights > 1.0] = 1.0  # Cap\n","    print(\"Class counts:\", class_counts)\n","    print(\"Class weights:\", class_weights)\n","\n","    label_encoder = LabelEncoder()\n","    label_encoder.classes_ = np.array([0, 1, 2])\n","\n","    data = construct_graph_knn(df, X, y, k=50)\n","    train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, random_state=42)\n","    data.train_mask = torch.zeros(len(y), dtype=torch.bool)\n","    data.test_mask = torch.zeros(len(y), dtype=torch.bool)\n","    data.train_mask[train_idx] = True\n","    data.test_mask[test_idx] = True\n","\n","    input_dim = X.shape[1]\n","    hidden_dim = 64\n","    output_dim = 3\n","    model = GNNModel(input_dim, hidden_dim, output_dim)  # Updated model\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","    train_model(data, model, optimizer, criterion, num_epochs=100)\n","    evaluate_model(data, model, label_encoder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fnm0g16e19ge","executionInfo":{"status":"ok","timestamp":1743418347391,"user_tz":-480,"elapsed":1209842,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}},"outputId":"1215d38e-559d-4e44-d2a6-b401266f8393","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial rows: 711705\n","Columns in the dataset: ['AskPrice1', 'AskSize1', 'BidPrice1', 'BidSize1', 'AskPrice2', 'AskSize2', 'BidPrice2', 'BidSize2', 'AskPrice3', 'AskSize3', 'BidPrice3', 'BidSize3', 'AskPrice4', 'AskSize4', 'BidPrice4', 'BidSize4', 'AskPrice5', 'AskSize5', 'BidPrice5', 'BidSize5', 'MidPrice', 'Time', 'MidPriceChange', 'Spread', 'Imbalance1', 'Imbalance2', 'Imbalance3', 'Imbalance4', 'Imbalance5', 'AskPriceDiff1', 'BidPriceDiff1', 'AskPriceDiff2', 'BidPriceDiff2', 'AskPriceDiff3', 'BidPriceDiff3', 'AskPriceDiff4', 'BidPriceDiff4', 'MidPriceChange_Lag1', 'MidPriceChange_Lag5', 'MidPriceChange_Lag10', 'EventCount_Type1', 'EventCount_Type2', 'EventCount_Type3', 'EventCount_Type4', 'EventCount_Type5', 'MidPrice_Volatility_10', 'CumulativeOrderFlow', 'MidPrice_MA10', 'AskSize1_MA10', 'BidSize1_MA10', 'TimeWeightedImbalance1', 'EventIntensity', 'AskPrice1_Relative', 'BidPrice1_Relative', 'AskPrice2_Relative', 'BidPrice2_Relative', 'AskPrice3_Relative', 'BidPrice3_Relative', 'AskPrice4_Relative', 'BidPrice4_Relative', 'AskPrice5_Relative', 'BidPrice5_Relative', 'TotalAskSize', 'TotalBidSize', 'DepthRatio', 'MidPriceChange_Cumsum10', 'MidPrice_EMA5', 'MidPrice_EMA20', 'AskVWAP', 'BidVWAP', 'VWAP_Imbalance', 'LogReturn', 'RealizedVol_1sec', 'TimeSinceOpen', 'Movement']\n","Unique values in 'Movement' (before): ['q_0' 'q_-1' 'q_+1']\n","Rows dropped due to invalid 'Movement': 0\n","Rows after subsampling: 177926\n","Unique values in y: [0 1 2]\n","Class counts: [59309 59309 59308]\n","Class weights: tensor([0.3333, 0.3333, 0.3333])\n","Epoch 0, Train Loss: 1.1089, Test Loss: 1.1080\n","Epoch 10, Train Loss: 1.0869, Test Loss: 1.0870\n","Epoch 20, Train Loss: 1.0798, Test Loss: 1.0806\n","Epoch 30, Train Loss: 1.0760, Test Loss: 1.0766\n","Epoch 40, Train Loss: 1.0724, Test Loss: 1.0733\n","Epoch 50, Train Loss: 1.0688, Test Loss: 1.0697\n","Epoch 60, Train Loss: 1.0648, Test Loss: 1.0655\n","Epoch 70, Train Loss: 1.0612, Test Loss: 1.0619\n","Epoch 80, Train Loss: 1.0583, Test Loss: 1.0585\n","Epoch 90, Train Loss: 1.0567, Test Loss: 1.0564\n","GNN (Train) Performance:\n","Balanced Accuracy: 0.4369\n","Weighted Precision: 0.4368\n","Weighted Recall: 0.4369\n","Weighted F1: 0.4366\n","MCC: 0.1555\n","Cohen Kappa: 0.1554\n","Precision_0: 0.4376\n","Recall_0: 0.4331\n","F1_0: 0.4353\n","Precision_1: 0.4326\n","Recall_1: 0.4086\n","F1_1: 0.4203\n","Precision_2: 0.4401\n","Recall_2: 0.4691\n","F1_2: 0.4542\n","\n","GNN (Test) Performance:\n","Balanced Accuracy: 0.4363\n","Weighted Precision: 0.4362\n","Weighted Recall: 0.4365\n","Weighted F1: 0.4361\n","MCC: 0.1547\n","Cohen Kappa: 0.1546\n","Precision_0: 0.4469\n","Recall_0: 0.4445\n","F1_0: 0.4457\n","Precision_1: 0.4217\n","Recall_1: 0.4003\n","F1_1: 0.4107\n","Precision_2: 0.4397\n","Recall_2: 0.4641\n","F1_2: 0.4516\n","\n"]}]},{"cell_type":"code","source":["# Load balanced dataset\n","data_path = \"/content/drive/MyDrive/FYP_Dataset/LOBSTER_SampleFile_AAPL_2012-06-21_5/AAPL_2012-06-21_balanced_dataset.csv\"\n","df, X, y = load_and_preprocess_data(data_path, sample_fraction=0.1)  # Smaller sample for speed\n","\n","# Enhanced graph construction\n","feature_cols = ['Time', 'MidPriceChange_Cumsum10', 'Imbalance1']\n","combined = StandardScaler().fit_transform(df[feature_cols])\n","nbrs = NearestNeighbors(n_neighbors=21).fit(combined)\n","distances, indices = nbrs.kneighbors(combined)\n","edge_index = torch.tensor([[i, j] for i in range(len(df)) for j in indices[i, 1:]], dtype=torch.long).t()\n","data = Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index, y=torch.tensor(y, dtype=torch.long))\n","\n","# Split\n","train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y, random_state=42)\n","data.train_mask = torch.zeros(len(y), dtype=torch.bool); data.train_mask[train_idx] = True\n","data.test_mask = torch.zeros(len(y), dtype=torch.bool); data.test_mask[test_idx] = True\n","\n","# Model with LSTM\n","class GNNWithLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n","        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, output_dim)\n","    def forward(self, data):\n","        x = data.x.unsqueeze(0)\n","        x, _ = self.lstm(x)\n","        x = x.squeeze(0)\n","        x = F.relu(self.conv1(x, data.edge_index))\n","        x = F.dropout(x, p=0.3, training=self.training)\n","        x = self.conv2(x, data.edge_index)\n","        return x\n","\n","# Train\n","model = GNNWithLSTM(X.shape[1], 128, 3)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","class_weights = 1.0 / torch.tensor(np.bincount(y), dtype=torch.float)\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n","train_model(data, model, optimizer, criterion, num_epochs=200)\n","evaluate_model(data, model, LabelEncoder().fit([0, 1, 2]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpAXt5PJME5z","executionInfo":{"status":"ok","timestamp":1743443533344,"user_tz":-480,"elapsed":1333441,"user":{"displayName":"Kai Darren","userId":"12740578643578893856"}},"outputId":"278dce35-2824-464f-c04b-0feb255ee9b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial rows: 711705\n","Columns in the dataset: ['AskPrice1', 'AskSize1', 'BidPrice1', 'BidSize1', 'AskPrice2', 'AskSize2', 'BidPrice2', 'BidSize2', 'AskPrice3', 'AskSize3', 'BidPrice3', 'BidSize3', 'AskPrice4', 'AskSize4', 'BidPrice4', 'BidSize4', 'AskPrice5', 'AskSize5', 'BidPrice5', 'BidSize5', 'MidPrice', 'Time', 'MidPriceChange', 'Spread', 'Imbalance1', 'Imbalance2', 'Imbalance3', 'Imbalance4', 'Imbalance5', 'AskPriceDiff1', 'BidPriceDiff1', 'AskPriceDiff2', 'BidPriceDiff2', 'AskPriceDiff3', 'BidPriceDiff3', 'AskPriceDiff4', 'BidPriceDiff4', 'MidPriceChange_Lag1', 'MidPriceChange_Lag5', 'MidPriceChange_Lag10', 'EventCount_Type1', 'EventCount_Type2', 'EventCount_Type3', 'EventCount_Type4', 'EventCount_Type5', 'MidPrice_Volatility_10', 'CumulativeOrderFlow', 'MidPrice_MA10', 'AskSize1_MA10', 'BidSize1_MA10', 'TimeWeightedImbalance1', 'EventIntensity', 'AskPrice1_Relative', 'BidPrice1_Relative', 'AskPrice2_Relative', 'BidPrice2_Relative', 'AskPrice3_Relative', 'BidPrice3_Relative', 'AskPrice4_Relative', 'BidPrice4_Relative', 'AskPrice5_Relative', 'BidPrice5_Relative', 'TotalAskSize', 'TotalBidSize', 'DepthRatio', 'MidPriceChange_Cumsum10', 'MidPrice_EMA5', 'MidPrice_EMA20', 'AskVWAP', 'BidVWAP', 'VWAP_Imbalance', 'LogReturn', 'RealizedVol_1sec', 'TimeSinceOpen', 'Movement']\n","Unique values in 'Movement' (before): ['q_0' 'q_-1' 'q_+1']\n","Rows dropped due to invalid 'Movement': 0\n","Rows after subsampling: 71170\n","Unique values in y: [0 1 2]\n","Epoch 0, Train Loss: 1.0983, Test Loss: 1.0977\n","Epoch 10, Train Loss: 1.0551, Test Loss: 1.0565\n","Epoch 20, Train Loss: 1.0029, Test Loss: 1.0058\n","Epoch 30, Train Loss: 0.9670, Test Loss: 0.9689\n","Epoch 40, Train Loss: 0.9387, Test Loss: 0.9402\n","Epoch 50, Train Loss: 0.9181, Test Loss: 0.9189\n","Epoch 60, Train Loss: 0.9059, Test Loss: 0.9075\n","Epoch 70, Train Loss: 0.8951, Test Loss: 0.8963\n","Epoch 80, Train Loss: 0.8851, Test Loss: 0.8863\n","Epoch 90, Train Loss: 0.8756, Test Loss: 0.8774\n","Epoch 100, Train Loss: 0.8675, Test Loss: 0.8688\n","Epoch 110, Train Loss: 0.8636, Test Loss: 0.8645\n","Epoch 120, Train Loss: 0.8607, Test Loss: 0.8617\n","Epoch 130, Train Loss: 0.8583, Test Loss: 0.8591\n","Epoch 140, Train Loss: 0.8567, Test Loss: 0.8575\n","Epoch 150, Train Loss: 0.8558, Test Loss: 0.8566\n","Epoch 160, Train Loss: 0.8546, Test Loss: 0.8555\n","Epoch 170, Train Loss: 0.8535, Test Loss: 0.8542\n","Epoch 180, Train Loss: 0.8537, Test Loss: 0.8543\n","Epoch 190, Train Loss: 0.8522, Test Loss: 0.8533\n","GNN (Train) Performance:\n","Balanced Accuracy: 0.6192\n","Weighted Precision: 0.6197\n","Weighted Recall: 0.6192\n","Weighted F1: 0.6062\n","MCC: 0.4372\n","Cohen Kappa: 0.4288\n","Precision_0: 0.6136\n","Recall_0: 0.7496\n","F1_0: 0.6748\n","Precision_1: 0.6215\n","Recall_1: 0.3801\n","F1_1: 0.4717\n","Precision_2: 0.6240\n","Recall_2: 0.7279\n","F1_2: 0.6720\n","\n","GNN (Test) Performance:\n","Balanced Accuracy: 0.6188\n","Weighted Precision: 0.6200\n","Weighted Recall: 0.6188\n","Weighted F1: 0.6051\n","MCC: 0.4371\n","Cohen Kappa: 0.4282\n","Precision_0: 0.6172\n","Recall_0: 0.7469\n","F1_0: 0.6759\n","Precision_1: 0.6258\n","Recall_1: 0.3743\n","F1_1: 0.4684\n","Precision_2: 0.6169\n","Recall_2: 0.7352\n","F1_2: 0.6709\n","\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOs7KLwl2KGszbMkHwTkkLD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}